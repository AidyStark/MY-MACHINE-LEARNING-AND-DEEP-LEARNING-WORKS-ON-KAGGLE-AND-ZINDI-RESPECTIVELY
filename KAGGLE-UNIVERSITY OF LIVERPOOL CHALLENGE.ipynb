{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KAGGLE-UNIVERSITY OF LIVERPOOL CHALLENGE",
      "provenance": [],
      "authorship_tag": "ABX9TyPpPTX7RJ021MMSl2gPkgqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adeyinka-hub/MY-MACHINE-LEARNING-AND-DEEP-LEARNING-WORKS-ON-KAGGLE-AND-ZINDI-RESPECTIVELY/blob/master/KAGGLE-UNIVERSITY%20OF%20LIVERPOOL%20CHALLENGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz7-BLycqjsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tsfresh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv9VrffHmSkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n",
        "from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\n",
        "from scipy.signal import butter, lfilter,filtfilt,savgol_filter\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tsfresh.feature_extraction import feature_calculators\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pandas_profiling import ProfileReport\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from contextlib import contextmanager\n",
        "from joblib import Parallel, delayed\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import itertools\n",
        "import warnings\n",
        "import librosa\n",
        "import time\n",
        "import pywt\n",
        "import os\n",
        "import gc\n",
        "\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJyuNUezm5DG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCHSIZE = 50000\n",
        "SEED = 529\n",
        "SELECT = True\n",
        "SPLITS = 5\n",
        "fe_config = [\n",
        "    (True, True, 50000, None),\n",
        "    (False, False, 5000, None),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxcSlYepm_8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_logger():\n",
        "    handler = StreamHandler()\n",
        "    handler.setLevel(INFO)\n",
        "    handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n",
        "    fh_handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    logger.setLevel(INFO)\n",
        "    logger.addHandler(handler)\n",
        "    logger.addHandler(fh_handler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Ak3UJznHRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@contextmanager\n",
        "def timer(name : Text):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "COMPETITION = 'ION-Switching'\n",
        "logger = getLogger(COMPETITION)\n",
        "LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
        "MODELNAME = 'Baseline'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKidEpawnMuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed : int) -> NoReturn :\n",
        "    \n",
        "    rn.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OGUNzRknSiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \n",
        "    train = pd.read_csv(os.path.join(base + '/train.csv'), dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int8})\n",
        "    test  = pd.read_csv(os.path.join(base + '/test.csv'), dtype={'time': np.float32, 'signal': np.float32})\n",
        "    sub  = pd.read_csv(os.path.join(base + '/sample_submission.csv'), dtype={'time': np.float32})\n",
        "    \n",
        "    return train, test, sub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4da3C9qMnZc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batching(df : pd.DataFrame,\n",
        "             batch_size : int,\n",
        "             add_index : Optional[bool]=True) -> pd.DataFrame :\n",
        "    \n",
        "    df['batch_'+ str(batch_size)] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values + 1\n",
        "    df['batch_'+ str(batch_size)] = df['batch_'+ str(batch_size)].astype(np.uint16)\n",
        "    if add_index:\n",
        "        df['batch_' + str(batch_size) +'_idx'] = df.index  - (df['batch_'+ str(batch_size)] * batch_size)\n",
        "        df['batch_' + str(batch_size) +'_idx'] = df['batch_' + str(batch_size) +'_idx'].astype(np.uint16)\n",
        "        \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E26AONfDnegV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df: pd.DataFrame,\n",
        "                     verbose: bool = True) -> pd.DataFrame:\n",
        "    \n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if (c_min > np.iinfo(np.int8).min\n",
        "                        and c_max < np.iinfo(np.int8).max):\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif (c_min > np.iinfo(np.int16).min\n",
        "                      and c_max < np.iinfo(np.int16).max):\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif (c_min > np.iinfo(np.int32).min\n",
        "                      and c_max < np.iinfo(np.int32).max):\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif (c_min > np.iinfo(np.int64).min\n",
        "                      and c_max < np.iinfo(np.int64).max):\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                 if (c_min > np.finfo(np.float16).min\n",
        "                        and c_max < np.finfo(np.float16).max):\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                 elif (c_min > np.finfo(np.float32).min\n",
        "                      and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                 else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "                end_mem = df.memory_usage().sum() / 1024**2\n",
        "                reduction = (start_mem - end_mem) / start_mem\n",
        "\n",
        "                msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
        "                if verbose:\n",
        "                 print(msg)\n",
        "\n",
        "                return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMHxVzXCn6if",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maddest(d : Union[np.array, pd.Series, List], axis : Optional[int]=None) -> np.array:  \n",
        "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
        "def denoise_signal(x : Union[np.array, pd.Series],\n",
        "                   wavelet : Optional[Text]='db4',\n",
        "                   level : Optional[int]=1) -> np.array:\n",
        "    \n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
        "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
        "\n",
        "    return pywt.waverec(coeff, wavelet, mode='per')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZLuehLbn75r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denoise_signal_simple(x : Union[np.array, pd.Series],\n",
        "                          wavelet : Optional[Text]='db4',\n",
        "                          level : Optional[int]=1) -> np.array:\n",
        "    \n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    uthresh = 10\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
        "    \n",
        "    return pywt.waverec(coeff, wavelet, mode='per')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi5pNK4boAdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trend(df : Union[pd.Series, np.array],\n",
        "          abs_values: Optional[bool]=False) -> float:\n",
        "    \n",
        "    idx = np.array(range(len(df)))\n",
        "    if abs_values:\n",
        "        df = np.abs(df)\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(idx.reshape(-1, 1), df)\n",
        "    \n",
        "    return lr.coef_[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYS4pRGwoF4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def change_rate(df : Union[pd.Series, np.array]) -> float:\n",
        "    \n",
        "    change = (np.diff(df) / df[:-1])\n",
        "    change = change[np.nonzero(change)[0]]\n",
        "    change = change[~np.isnan(change)]\n",
        "    change = change[change != -np.inf]\n",
        "    change = change[change != np.inf]\n",
        "    \n",
        "    return np.mean(change)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLE9R549oKkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lag_with_pct_change(df : pd.DataFrame,\n",
        "                        batch_size : int,\n",
        "                        shift_sizes : Optional[List]=[1, 2],\n",
        "                        add_pct_change : Optional[bool]=False,\n",
        "                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n",
        "    \n",
        "    assert 'batch_' + str(batch_size) +'_idx' in df.columns\n",
        "    for shift_size in shift_sizes:    \n",
        "        df['signal_shift_pos_'+str(shift_size)] = df['signal'].shift(shift_size).fillna(0)\n",
        "        df['signal_shift_neg_'+str(shift_size)] = df['signal'].shift(-1*shift_size).fillna(0)\n",
        "    for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(shift_size))].index:\n",
        "            df['signal_shift_pos_'+str(shift_size)][i] = np.nan\n",
        "    for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(batch_size - shift_size, batch_size))].index:\n",
        "            df['signal_shift_neg_'+str(shift_size)][i] = np.nan\n",
        "    if add_pct_change:\n",
        "        df['pct_change'] = df['signal'].pct_change()\n",
        "    if add_pct_change_lag:\n",
        "            df['pct_change_shift_pos_'+str(shift_size)] = df['pct_change'].shift(shift_size).fillna(0)\n",
        "            df['pct_change_shift_neg_'+str(shift_size)] = df['pct_change'].shift(-1*shift_size).fillna(0)\n",
        "    for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(shift_size))].index:\n",
        "                df['pct_change_shift_pos_'+str(shift_size)][i] = np.nan\n",
        "    for i in df[df['batch_' + str(batch_size) +'_idx'].isin(range(batch_size - shift_size, batch_size))].index:\n",
        "                df['pct_change_shift_neg_'+str(shift_size)][i] = np.nan \n",
        "    return df                                                                              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpadGwu6ooeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_enginering_by_batch(z : Union[pd.Series, np.array],\n",
        "                                batch_size : int,\n",
        "                                window_size : Optional[List]=None) -> pd.DataFrame:\n",
        "    \n",
        "    temp = pd.DataFrame(index=[0], dtype=np.float16)\n",
        "    \n",
        "    temp['mean'] = z.mean()\n",
        "    temp['max'] = z.max()\n",
        "    temp['min'] = z.min()\n",
        "    temp['std'] = z.std()  \n",
        "    temp['mean_abs_chg'] = np.mean(np.abs(np.diff(z)))\n",
        "    temp['abs_max'] = np.max(np.abs(z))\n",
        "    temp['abs_min'] = np.min(np.abs(z))\n",
        "    temp['range'] = temp['max'] - temp['min']\n",
        "    temp['max_to_min'] = temp['max'] / temp['min']\n",
        "    temp['abs_avg'] = (temp['abs_max'] + temp['abs_min']) / 2\n",
        "    \n",
        "    for i in range(1, 5): \n",
        "        temp[f'kstat_{i}'] = stats.kstat(z, i)\n",
        "\n",
        "    for i in range(2, 5):\n",
        "        temp[f'moment_{i}'] = stats.moment(z, i)\n",
        "\n",
        "    for i in [1, 2]:\n",
        "        temp[f'kstatvar_{i}'] = stats.kstatvar(z, i)\n",
        "    \n",
        "    if window_size is not None:\n",
        "        for window in window_size:\n",
        "            temp['percentile_roll_'+str(window)+'_std_25'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values, 25)\n",
        "            temp['percentile_roll_'+str(window)+'_std_75'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values, 75)\n",
        "            temp['percentile_roll_'+str(window)+'_std_05'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values,  5)\n",
        "            temp['percentile_roll_'+str(window)+'_std_95'] = np.percentile(pd.Series(z).rolling(window).std().dropna().values, 95)\n",
        "            temp['percentile_roll_'+str(window)+'_mean_25'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values, 25)\n",
        "            temp['percentile_roll_'+str(window)+'_mean_75'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values, 75)\n",
        "            temp['percentile_roll_'+str(window)+'_mean_05'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values,  5)\n",
        "            temp['percentile_roll_'+str(window)+'_mean_95'] = np.percentile(pd.Series(z).rolling(window).mean().dropna().values, 95)            \n",
        "    return temp  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPJuB4V-ozOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_sample(sample : pd.DataFrame,\n",
        "                 batch_no : int,\n",
        "                 batch_size : int,\n",
        "                 window_size : List) -> pd.DataFrame:\n",
        "    \n",
        "    temp = feature_enginering_by_batch(sample['signal'].values, batch_size, window_size)\n",
        "    temp['batch_'+ str(batch_size)] = int(batch_no)\n",
        "    \n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElBg5rH_o6T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_gen(df : pd.DataFrame,\n",
        "               batch_size : int,\n",
        "               window_size : List,\n",
        "               batches : List=[0], ) -> pd.DataFrame:\n",
        "    \n",
        "    result = Parallel(n_jobs=1, temp_folder='/tmp', max_nbytes=None, backend='multiprocessing')(delayed(parse_sample)\n",
        "                                              (df[df['batch_'+ str(batch_size)]==i], int(i), batch_size, window_size)\n",
        "                                                                                              for i in tqdm(batches))\n",
        "    data = [r.values for r in result]\n",
        "    data = np.vstack(data)\n",
        "    cols = result[0].columns\n",
        "    cols = [name+'_'+str(batch_size) if name!='batch_'+ str(batch_size) else 'batch_'+ str(batch_size) for name in cols ]\n",
        "    X = pd.DataFrame(data, columns=cols)\n",
        "    X = reduce_mem_usage(X, False)\n",
        "    X = X.sort_values('batch_'+ str(batch_size))\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSdcedDxpCmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_feat_enginnering(df : pd.DataFrame,\n",
        "                         create_all_data_feats : bool,\n",
        "                         add_index : bool,\n",
        "                         batch_size : int,\n",
        "                         window_size : List) -> pd.DataFrame:\n",
        "    \n",
        "    df = batching(df, batch_size=batch_size, add_index=add_index)\n",
        "    if create_all_data_feats:\n",
        "        df = lag_with_pct_change(df, batch_size, [1, 2, 4],  add_pct_change=True, add_pct_change_lag=True)\n",
        "    batches = df['batch_'+ str(batch_size)].unique().tolist()\n",
        "    batch_feats=sample_gen(df, batch_size=batch_size, window_size=window_size, batches=batches)\n",
        "    df = pd.merge(df, batch_feats, on='batch_'+ str(batch_size), how='left')\n",
        "    df = reduce_mem_usage(df, False)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mikSVRY6pH1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_selection(df : pd.DataFrame,\n",
        "                      df_test : pd.DataFrame,\n",
        "                      subtract_only : Optional[bool]=True,\n",
        "                      idx_cols : List=['time'],\n",
        "                      target_col : List=['open_channels']) -> Tuple[pd.DataFrame , pd.DataFrame]:\n",
        "    \n",
        "    drops = df.columns[df.isna().sum()>25000]\n",
        "    df = df.drop(drops, axis=1)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df = df.fillna(0)\n",
        "    gc.collect()\n",
        "    if subtract_only == False:\n",
        "        corrcoef_cols = [col for col in df.columns.tolist() if col not in (idx_cols+target_col)]\n",
        "        first=dict(); second=dict(); third=dict()\n",
        "        for col in corrcoef_cols:\n",
        "            ss = np.corrcoef(df[col], df['open_channels'])[0, 1]\n",
        "            first[col] = ss\n",
        "            ss = np.corrcoef(df[col]-df['signal'], df['open_channels'])[0, 1]\n",
        "            second[col] = ss\n",
        "            ss = np.corrcoef(df[col]*df['signal'], df['open_channels'])[0, 1]\n",
        "            third[col] = ss\n",
        "        corr_df = pd.DataFrame.from_dict(\n",
        "            {\n",
        "            'Base':first, \n",
        "            'Signal-Subtracted': second,\n",
        "            'Signal-Multiplied': third\n",
        "            }\n",
        "        ).fillna(0).apply(np.abs).sort_values('Base', ascending=False)\n",
        "\n",
        "        base_cols = corr_df.sort_values('Base', ascending=False).head(100).index.tolist()\n",
        "        multiply_cols = corr_df.sort_values('Signal-Multiplied', ascending=False).head(10).index.tolist()\n",
        "        subtract_cols = corr_df.sort_values('Signal-Subtracted', ascending=False).head(25).index.tolist()\n",
        "        display(corr_df.sort_values('Base', ascending=False).tail(50))\n",
        "        all_cols = list(set(base_cols + multiply_cols + subtract_cols + idx_cols + target_col))\n",
        "        all_cols_test = list(set(base_cols + multiply_cols + subtract_cols + idx_cols))   \n",
        "        drops = list(set(multiply_cols + subtract_cols)-set(base_cols))\n",
        "        df = df[all_cols]\n",
        "        df_test = df_test[all_cols_test]\n",
        "    \n",
        "        for col in multiply_cols:\n",
        "            df[col+'_m'] = df[col] * df['signal']\n",
        "            df_test[col+'_m'] = df_test[col] * df_test['signal']        \n",
        "        for col in subtract_cols:\n",
        "            df[col+'_s'] = df[col] - df['signal']\n",
        "            df_test[col+'_s'] = df_test[col] - df_test['signal']\n",
        "        df = df.drop(drops, axis=1)\n",
        "    else:\n",
        "       not_imp = ['kstat_1_5000', 'kstat_2_5000', 'kstat_3_5000', 'kstat_4_5000', 'moment_2_5000',\n",
        "                   'moment_3_5000','moment_4_5000', 'kstatvar_1_5000', 'kstatvar_2_5000','kstat_1_50000',\n",
        "                   'kstat_2_50000', 'kstat_3_50000', 'kstat_4_50000', 'moment_2_50000', 'moment_3_50000',\n",
        "                   'moment_4_50000', 'kstatvar_1_50000', 'kstatvar_2_50000']\n",
        "\n",
        "    subtract_cols = list(set(df.columns.tolist())-set(idx_cols + target_col + not_imp))\n",
        "    for col in subtract_cols:\n",
        "            df[col+'_s'] = df[col] - df['signal']\n",
        "            df_test[col+'_s'] = df_test[col] - df_test['signal']\n",
        "    df = reduce_mem_usage(df, False)\n",
        "    df_test = reduce_mem_usage(df_test, False)\n",
        "\n",
        "    gc.collect()\n",
        "    return df, df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4GHU96VpetW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MacroF1Metric(preds : np.array, dtrain : lgb.Dataset) -> Tuple[Text, np.array, bool] :\n",
        "    \n",
        "    labels = dtrain.get_label()\n",
        "    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n",
        "    score = f1_score(labels, preds, average = 'macro')\n",
        "    \n",
        "    return ('MacroF1Metric', score, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciUET8JupkOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_cv_model_by_batch(train : pd.DataFrame,\n",
        "                          test : pd.DataFrame,\n",
        "                          splits : int,\n",
        "                          shuffle : bool,\n",
        "                          seed : int,\n",
        "                          batch_col : Text,\n",
        "                          params : Dict,\n",
        "                          feats : List,\n",
        "                          sample_submission: pd.DataFrame) -> pd.DataFrame:\n",
        "    \n",
        "    oof_ = np.zeros(len(train))\n",
        "    preds_ = np.zeros(len(test))\n",
        "    target = ['open_channels']\n",
        "    imp_df = pd.DataFrame(index=feats)\n",
        "    kf = KFold(splits, shuffle, seed)\n",
        "    for n_fold, (tr_idx, val_idx) in enumerate(kf.split(train, train[target], groups=train[batch_col])):\n",
        "        tr_x = train[feats].iloc[tr_idx]\n",
        "        vl_x = train[feats].iloc[val_idx]\n",
        "        tr_y = train[target].iloc[tr_idx].values\n",
        "        vl_y = train[target].iloc[val_idx].values\n",
        "        model = HistGradientBoostingRegressor(learning_rate = 0.1, max_iter=800, random_state = 404, validation_fraction=None, verbose = 0, max_depth=12, min_samples_leaf=25, l2_regularization=0.05)\n",
        "        model.fit(tr_x, tr_y)\n",
        "        oof_[val_idx] += model.predict(train[feats].iloc[val_idx])\n",
        "        preds_ += model.predict(test[feats]) / SPLITS\n",
        "        f1_score_ = f1_score(train[target].iloc[val_idx], np.round(np.clip(oof_[val_idx], 0, 10)).astype(int), average = 'macro')\n",
        "        rmse_score_ = np.sqrt(mean_squared_error(train[target].iloc[val_idx], oof_[val_idx]))\n",
        "        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f} rmse score : {rmse_score_:1.5f}')\n",
        "        #imp_df[f'feat_importance_{n_fold + 1}'] = model.feature_importance(importance_type='gain')\n",
        "    f1_score_ = f1_score(train[target], np.round(np.clip(oof_, 0, 10)).astype(int), average = 'macro')\n",
        "    rmse_score_ = np.sqrt(mean_squared_error(train[target], oof_))\n",
        "    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f} oof rmse score : {rmse_score_:1.5f}')\n",
        "    sample_submission['open_channels'] = np.round(np.clip(preds_, 0, 10)).astype(int)\n",
        "    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n",
        "    display(sample_submission.head())\n",
        "    np.save('oof.npy', oof_)\n",
        "    np.save('preds.npy', preds_)\n",
        "\n",
        "    return imp_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdOE9kFlpvg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_params(seed : int) -> Dict :\n",
        "    params = dict()\n",
        "    params['learning_rate']=0.009;\n",
        "    params['max_depth']=-1;\n",
        "    params['num_leaves']=257;\n",
        "    params['metric']='rmse';\n",
        "    params['random_state']=seed;\n",
        "    params['n_jobs']=-1;\n",
        "    params['feature_fraction']=1 ;\n",
        "    params['boosting']='goss';\n",
        "    params['boost_from_average']=True;\n",
        "    params['bagging_seed']=seed;\n",
        "    params['bagging_freq']=0;\n",
        "    params['bagging_fraction']=1;\n",
        "    params['reg_alpha']=0;\n",
        "    params['reg_lambda']=0\n",
        "    params['force_row_wise']=True\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8wa9EIhp1eY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_everything(fe_config : List) -> NoReturn:\n",
        "    not_feats_cols = ['time']\n",
        "    target_col = ['open_channels']\n",
        "    init_logger()\n",
        "    with timer(f'Reading Data'):\n",
        "        logger.info('Reading Data Started ...')\n",
        "        base = os.path.abspath('/kaggle/input/liverpool-ion-switching/')\n",
        "        train, test, sample_submission = read_data(base)\n",
        "        logger.info('Reading Data Completed ...')\n",
        "        \n",
        "    with timer(f'Creating Features'):\n",
        "        logger.info('Feature Enginnering Started ...')\n",
        "        for config in fe_config:\n",
        "            train = run_feat_enginnering(train, create_all_data_feats=config[0], add_index=config[1], batch_size=config[2], window_size=config[3])\n",
        "            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], add_index=config[1], batch_size=config[2], window_size=config[3])\n",
        "            not_feats_cols.append('batch_'+str(config[2]))\n",
        "            if config[1]:\n",
        "                not_feats_cols.append('batch_'+str(config[2])+'_idx')\n",
        "        if SELECT:\n",
        "            train, test = feature_selection(train, test, subtract_only=True, idx_cols=not_feats_cols, target_col=target_col)\n",
        "        logger.info('Feature Enginnering Completed ...')\n",
        "\n",
        "    with timer(f'Running HistGradientBoosting model'):\n",
        "        logger.info(f'Training HistGradientBoosting model with {SPLITS} folds Started ...')\n",
        "        params = get_params(SEED)\n",
        "        feats = [c for c in train.columns if c not in (not_feats_cols+target_col)]\n",
        "        importances = run_cv_model_by_batch(train, test, splits=SPLITS, shuffle=True, seed=SEED, batch_col='batch_50000',params=params, feats=feats, sample_submission=sample_submission)\n",
        "        importances.to_csv('importances.csv')\n",
        "        logger.info(f'Training completed ...')    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEKGcwTCqHpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_everything(fe_config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}